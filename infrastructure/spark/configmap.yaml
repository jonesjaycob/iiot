---
apiVersion: v1
kind: ConfigMap
metadata:
  name: spark-compaction-script
  namespace: industrial-iot
data:
  compact.py: |
    """
    Iceberg table compaction job.
    Rewrites small data files into larger optimized Parquet files
    and expires old snapshots to reclaim storage.
    """
    import os
    import sys
    from pyspark.sql import SparkSession

    NESSIE_URI = os.environ.get("NESSIE_URI", "http://nessie.industrial-iot.svc.cluster.local:19120/api/v2")
    S3_ENDPOINT = os.environ.get("S3_ENDPOINT", "http://minio.industrial-iot.svc.cluster.local:9000")
    WAREHOUSE = os.environ.get("WAREHOUSE", "s3://iceberg-warehouse/")
    TABLES = os.environ.get("TABLES", "manufacturing.opcua_telemetry").split(",")

    spark = SparkSession.builder \
        .appName("iceberg-compaction") \
        .config("spark.sql.extensions", "org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions") \
        .config("spark.sql.catalog.iceberg", "org.apache.iceberg.spark.SparkCatalog") \
        .config("spark.sql.catalog.iceberg.catalog-impl", "org.apache.iceberg.nessie.NessieCatalog") \
        .config("spark.sql.catalog.iceberg.uri", NESSIE_URI) \
        .config("spark.sql.catalog.iceberg.ref", "main") \
        .config("spark.sql.catalog.iceberg.warehouse", WAREHOUSE) \
        .config("spark.sql.catalog.iceberg.io-impl", "org.apache.iceberg.aws.s3.S3FileIO") \
        .config("spark.sql.catalog.iceberg.s3.endpoint", S3_ENDPOINT) \
        .config("spark.sql.catalog.iceberg.s3.path-style-access", "true") \
        .config("spark.hadoop.fs.s3a.endpoint", S3_ENDPOINT) \
        .config("spark.hadoop.fs.s3a.access.key", os.environ["AWS_ACCESS_KEY_ID"]) \
        .config("spark.hadoop.fs.s3a.secret.key", os.environ["AWS_SECRET_ACCESS_KEY"]) \
        .config("spark.hadoop.fs.s3a.path.style.access", "true") \
        .config("spark.hadoop.fs.s3a.impl", "org.apache.hadoop.fs.s3a.S3AFileSystem") \
        .getOrCreate()

    for table in TABLES:
        fqn = f"iceberg.{table.strip()}"
        print(f"Compacting {fqn}...")

        try:
            # Rewrite small data files into larger ones (target 256MB)
            spark.sql(f"""
                CALL iceberg.system.rewrite_data_files(
                    table => '{fqn}',
                    options => map('target-file-size-bytes', '268435456')
                )
            """)
            print(f"  Data files rewritten for {fqn}")

            # Expire snapshots older than 7 days
            spark.sql(f"""
                CALL iceberg.system.expire_snapshots(
                    table => '{fqn}',
                    older_than => TIMESTAMP '{{}}'.format(
                        (current_timestamp() - INTERVAL 7 DAYS)
                    ),
                    retain_last => 10
                )
            """)
            print(f"  Old snapshots expired for {fqn}")

            # Remove orphan files
            spark.sql(f"""
                CALL iceberg.system.remove_orphan_files(table => '{fqn}')
            """)
            print(f"  Orphan files removed for {fqn}")

        except Exception as e:
            print(f"  Warning: compaction failed for {fqn}: {e}", file=sys.stderr)
            # Continue with next table rather than failing the whole job

    print("Compaction complete.")
    spark.stop()
